{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nqWf4zrbfXOE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.resnet import ResNet50_Weights\n",
        "from torchvision.models.densenet import DenseNet161_Weights\n",
        "from torchvision.models.resnet import ResNeXt50_32X4D_Weights\n",
        "from torchvision.models.resnet import ResNeXt101_64X4D_Weights\n",
        "from torchvision.models.resnet import ResNet101_Weights\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "#Model Info\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.0005\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "TEST_BATCH_SIZE = 8\n",
        "\n",
        "EPOCHS = 20\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY = False\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/ism_project_2023/train\"\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/ism_project_2023/train.csv\"\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# Augmentation parameters\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_WIDTH = 224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSgO5iraolDC",
        "outputId": "1c84882f-a606-4638-8877-1144a0272e36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ColonCancerDataset(Dataset):\n",
        "    def __init__(self, image_dir, annotation_file, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "        self.csv = pd.read_csv(annotation_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):  #iterator over the dataset\n",
        "        \"\"\"\n",
        "        index: acts as an iterator over the dataset\n",
        "\n",
        "        return:\n",
        "        image: torch tensor of format [batch_size, height, width, channels]\n",
        "        label: torch tensor of integer type of format [batch_size, label_value]\n",
        "        \"\"\"\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, self.images[index])\n",
        "        #print(\"image_path: \", image_path)\n",
        "\n",
        "        #read image and labels\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        image_basename = os.path.splitext(os.path.basename(image_path))[0] #without '.jpg'\n",
        "        #print(\"image_basename: \", image_basename)\n",
        "\n",
        "        #check the basename present in the name column in csv file and get the corresponding label value\n",
        "        label = self.csv[self.csv['name'] == image_basename]['label'].values[0]\n",
        "        #print(\"label: \", label)\n",
        "\n",
        "        #applying augmentations\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image=image)\n",
        "            image = augmentations['image']\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XF9W2Uewox-g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Train():\n",
        "  def __init__(self) -> None:\n",
        "      print(\"Inside Training Loop\")\n",
        "\n",
        "  def train(self,trainloader, model, optimizer, loss_fn, DEVICE='cuda'):\n",
        "    loop = tqdm(trainloader, leave=True)\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (image, targets) in enumerate(loop):\n",
        "        image = image.to(DEVICE) #to GPU\n",
        "        targets = targets.to(DEVICE)\n",
        "        #print(\"data.shape: \", image.shape)\n",
        "        #print(\"targets.shape: \", targets.shape)\n",
        "        #time.sleep(10)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(image)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loop.set_postfix(loss=loss.item()) #loss over items in a single batch\n",
        "        running_loss += loss.item() #loss over all batches\n",
        "\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    return train_loss\n",
        "\n",
        "  def validation(self,validloader, model, loss_fn, DEVICE='cuda'):\n",
        "      loop = tqdm(validloader, leave=True)\n",
        "      model.eval()\n",
        "      valid_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for batch_idx, (image, targets) in enumerate(loop):\n",
        "              image = image.to(DEVICE)\n",
        "              targets = targets.to(DEVICE)\n",
        "\n",
        "              preds = model(image)\n",
        "              loss = loss_fn(preds, targets)\n",
        "              loop.set_postfix(loss=loss.item())\n",
        "              valid_loss += loss.item()\n",
        "\n",
        "      valid_loss = valid_loss / len(validloader)\n",
        "      return valid_loss"
      ],
      "metadata": {
        "id": "32xVIYOnpDQW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  #augmentations\n",
        "  transform = A.Compose(\n",
        "      [\n",
        "          A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH), #always use resizing to reduce computation\n",
        "          A.HorizontalFlip(p=0.5),\n",
        "          A.VerticalFlip(p=0.2),\n",
        "          A.Rotate(limit=[-30,30]),\n",
        "          A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #using imagenet mean and std\n",
        "          ToTensorV2(), #converting to the pytorch required format: [batch, channels, height, width]\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  if transform is not None:\n",
        "      print(\"INFO: Augmentations applied\")\n",
        "    #load the images and labels\n",
        "  dataset = ColonCancerDataset(\n",
        "      image_dir=TRAIN_IMG_DIR,\n",
        "      annotation_file=TRAIN_CSV,\n",
        "      transform=transform,\n",
        "  )\n",
        "\n",
        "  print(\"Dataset loaded\")\n",
        "\n",
        "  #split: 80% train, 20% validation\n",
        "  train_size = int(0.6 * len(dataset))\n",
        "  valid_size = len(dataset) - train_size\n",
        "  trainset, validset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "  print(f\"INFO: Training data split, TRAIN: {train_size}, VALID: {valid_size}\")\n",
        "\n",
        "  # Further split validation set into validation and test sets\n",
        "  valid_size = int(0.5 * len(validset))\n",
        "  test_size = len(validset) - valid_size\n",
        "  valid_dataset, test_dataset = torch.utils.data.random_split(validset, [valid_size, test_size])\n",
        "  print(f\"INFO: Training data split, TRAIN: {train_size}, VALID: {valid_size},TEST: {test_size}\")\n",
        "\n",
        "  #dataloaders\n",
        "  train_loader = DataLoader(\n",
        "      dataset=trainset,\n",
        "      batch_size=TRAIN_BATCH_SIZE,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=PIN_MEMORY,\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  valid_loader = DataLoader(\n",
        "      dataset=validset,\n",
        "      batch_size=VALID_BATCH_SIZE,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=PIN_MEMORY,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=TEST_BATCH_SIZE,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pin_memory=PIN_MEMORY,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  print(\"Train, Val, Test Loaded\")\n",
        "\n",
        "  #loading model and setting hyperparameters\n",
        "\n",
        "  model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
        "\n",
        "  model.fc = torch.nn.Linear(in_features=2048, out_features=NUM_CLASSES) #changing last layer\n",
        "  model.to(DEVICE)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "  loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  best_loss = 0.90\n",
        "  best_epoch = 0\n",
        "\n",
        "  print(model)\n",
        "  utils=Train()\n",
        "  print(\"INFO: Training started\")\n",
        "  for epoch in range(1, EPOCHS+1):\n",
        "      print(f\"Epoch: {epoch}/{EPOCHS}\")\n",
        "      train_loss = utils.train(train_loader, model, optimizer, loss_fn)\n",
        "      valid_loss = utils.validation(valid_loader, model, loss_fn)\n",
        "\n",
        "      print(f\"INFO: Training loss: {train_loss:.3f}\")\n",
        "      print(f\"INFO: Validation loss {valid_loss:.3f}\")\n",
        "\n",
        "      #saving best model based on losses\n",
        "      if valid_loss < best_loss:\n",
        "          best_loss = valid_loss\n",
        "          torch.save(model.state_dict(), \"best_resnext50_0.005_model.pth\")\n",
        "          best_epoch = epoch\n",
        "          print(f\"Best model saved at epoch {best_epoch} with loss: {best_loss:.3f}\")\n",
        "\n",
        "  print(\"INFO: Training completed\")\n",
        "\n",
        "  #test_model\n",
        "\n",
        "  print(\"Testing Started!\")\n",
        "  model.load_state_dict(torch.load(\"best_resnext50_model.pth\"))\n",
        "  model.to(DEVICE)\n",
        "  model.eval()\n",
        "  from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "  test_predictions = []\n",
        "  test_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "          test_predictions.extend(predicted.cpu().numpy())\n",
        "          test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  accuracy = accuracy_score(test_labels, test_predictions)\n",
        "  classification_report_result = classification_report(test_labels, test_predictions)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_report_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j4FAW4totYL",
        "outputId": "b551db6b-0601-4e36-8864-9512f105ff7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Augmentations applied\n",
            "Dataset loaded\n",
            "INFO: Training data split, TRAIN: 2412, VALID: 1609\n",
            "INFO: Training data split, TRAIN: 2412, VALID: 804,TEST: 805\n",
            "Train, Val, Test Loaded\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
            ")\n",
            "Inside Training Loop\n",
            "INFO: Training started\n",
            "Epoch: 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:48<00:00,  2.77it/s, loss=1.07]\n",
            "100%|██████████| 202/202 [00:33<00:00,  6.02it/s, loss=1.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.987\n",
            "INFO: Validation loss 0.975\n",
            "Epoch: 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:09<00:00,  4.32it/s, loss=0.701]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.16it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.852\n",
            "INFO: Validation loss 0.695\n",
            "Best model saved at epoch 2 with loss: 0.695\n",
            "Epoch: 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:09<00:00,  4.34it/s, loss=1.84]\n",
            "100%|██████████| 202/202 [00:31<00:00,  6.32it/s, loss=1.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.827\n",
            "INFO: Validation loss 0.614\n",
            "Best model saved at epoch 3 with loss: 0.614\n",
            "Epoch: 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.28it/s, loss=1.4]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.15it/s, loss=1.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.738\n",
            "INFO: Validation loss 0.663\n",
            "Epoch: 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.26it/s, loss=0.229]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.24it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.741\n",
            "INFO: Validation loss 0.678\n",
            "Epoch: 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.27it/s, loss=0.714]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.31it/s, loss=1.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.699\n",
            "INFO: Validation loss 0.659\n",
            "Epoch: 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.26it/s, loss=0.871]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.22it/s, loss=0.874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.700\n",
            "INFO: Validation loss 0.663\n",
            "Epoch: 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.27it/s, loss=0.542]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.24it/s, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.646\n",
            "INFO: Validation loss 0.704\n",
            "Epoch: 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.27it/s, loss=0.567]\n",
            "100%|██████████| 202/202 [00:31<00:00,  6.35it/s, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.643\n",
            "INFO: Validation loss 0.600\n",
            "Best model saved at epoch 9 with loss: 0.600\n",
            "Epoch: 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.29it/s, loss=1.06]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.24it/s, loss=0.967]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.652\n",
            "INFO: Validation loss 0.645\n",
            "Epoch: 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:09<00:00,  4.34it/s, loss=0.451]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.28it/s, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.621\n",
            "INFO: Validation loss 0.627\n",
            "Epoch: 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:09<00:00,  4.35it/s, loss=0.328]\n",
            "100%|██████████| 202/202 [00:31<00:00,  6.37it/s, loss=3.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.623\n",
            "INFO: Validation loss 0.755\n",
            "Epoch: 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.31it/s, loss=0.523]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.31it/s, loss=2.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.620\n",
            "INFO: Validation loss 0.621\n",
            "Epoch: 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.26it/s, loss=0.503]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.27it/s, loss=1.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.623\n",
            "INFO: Validation loss 0.651\n",
            "Epoch: 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.27it/s, loss=0.581]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.30it/s, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.599\n",
            "INFO: Validation loss 0.634\n",
            "Epoch: 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.27it/s, loss=0.33]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.22it/s, loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.571\n",
            "INFO: Validation loss 0.618\n",
            "Epoch: 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.27it/s, loss=1.13]\n",
            "100%|██████████| 202/202 [00:32<00:00,  6.27it/s, loss=0.69]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.577\n",
            "INFO: Validation loss 0.606\n",
            "Epoch: 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:10<00:00,  4.29it/s, loss=0.372]\n",
            "100%|██████████| 202/202 [00:31<00:00,  6.34it/s, loss=1.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.581\n",
            "INFO: Validation loss 0.644\n",
            "Epoch: 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:09<00:00,  4.36it/s, loss=0.451]\n",
            "100%|██████████| 202/202 [00:31<00:00,  6.36it/s, loss=1.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.539\n",
            "INFO: Validation loss 0.592\n",
            "Best model saved at epoch 19 with loss: 0.592\n",
            "Epoch: 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 302/302 [01:09<00:00,  4.35it/s, loss=0.502]\n",
            "100%|██████████| 202/202 [00:31<00:00,  6.40it/s, loss=2.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Training loss: 0.538\n",
            "INFO: Validation loss 0.658\n",
            "INFO: Training completed\n",
            "Testing Started!\n",
            "Accuracy: 0.8807\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90       218\n",
            "           1       0.72      0.85      0.78       176\n",
            "           2       0.98      0.96      0.97       288\n",
            "           3       0.88      0.73      0.80       123\n",
            "\n",
            "    accuracy                           0.88       805\n",
            "   macro avg       0.87      0.86      0.86       805\n",
            "weighted avg       0.89      0.88      0.88       805\n",
            "\n"
          ]
        }
      ]
    }
  ]
}